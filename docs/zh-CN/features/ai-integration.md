# AI 集成

连接多个 AI 供应商，并在整个 PackageFlow 中使用智能辅助。

## 概览

PackageFlow 支持多个 AI 供应商用于智能功能，如：

- 提交信息生成
- 代码分析
- 安全公告摘要
- 自定义 AI 提示

<!-- TODO: Add screenshot of AI settings panel -->

## 支持的供应商

| 供应商 | 模型 | 验证 |
|--------|------|------|
| **OpenAI** | GPT-3.5、GPT-4、GPT-4 Turbo | API Key |
| **Anthropic** | Claude 3 系列 | API Key |
| **Google** | Gemini Pro、Gemini Ultra | API Key |
| **Ollama** | 任何本地模型 | 本地 |
| **LM Studio** | 任何本地模型 | 本地 |

## 添加 AI 服务

### 云端供应商（OpenAI、Anthropic、Google）

1. 前往**设置** → **AI 服务**
2. 点击**添加服务**
3. 选择供应商
4. 输入您的 API key
5. 点击**验证并保存**

<!-- TODO: Add screenshot of add service dialog -->

### 本地供应商（Ollama、LM Studio）

1. 确保 Ollama/LM Studio 在本地运行
2. 前往**设置** → **AI 服务**
3. 点击**添加服务**
4. 选择 **Ollama** 或 **LM Studio**
5. 输入本地 URL（默认：`http://localhost:11434`）
6. 点击**连接**

## API Key 安全

API key 使用 AES-256-GCM 加密并安全存储：

- Key 永远不会暴露在日志中
- 静态加密
- 存储在系统 keychain（macOS）

## 选择模型

### 每个服务的模型

每个服务都有可用模型：

1. 点击服务
2. 点击**获取模型**
3. 选择您偏好的默认模型

### 每个功能的模型

为不同任务选择不同模型：

- 提交信息：较快的模型（GPT-3.5）
- 代码审查：更强大的模型（GPT-4）

## AI 功能

### 提交信息生成

从您的差异生成有意义的提交信息：

1. 暂存您的变更
2. 点击提交表单中的 **AI** 按钮
3. AI 分析差异并生成信息
4. 需要时编辑，然后提交

<!-- TODO: Add gif of AI commit message generation -->

### 代码分析（即将推出）

AI 驱动的代码审查建议。

### 安全摘要（即将推出）

安全漏洞的白话解释。

## 提示模板

使用模板自定义 AI 生成内容的方式。

### 默认模板

PackageFlow 包含以下模板：

- Git 提交信息
- Pull request 描述
- 代码审查评论
- 发布说明
- 安全公告
- 自定义

### 创建自定义模板

1. 前往**设置** → **AI 服务** → **模板**
2. 点击**新建模板**
3. 配置：
   - 名称
   - 类别
   - 带变量的提示文本
4. 保存

<!-- TODO: Add screenshot of template editor -->

### 模板变量

在提示中使用变量：

| 变量 | 说明 |
|------|------|
| `{diff}` | Git 差异内容 |
| `{code}` | 选中的代码 |
| `{file_path}` | 当前文件路径 |
| `{language}` | 文件语言 |
| `{project_name}` | 项目名称 |

### 模板示例

**提交信息模板：**

```
根据以下 git diff，生成遵循 conventional commit 格式的简洁提交信息。

重点：
- 变更了什么（而非如何）
- 为什么变更（如果明显）
- 第一行保持在 72 个字符以下

Diff：
{diff}
```

### 项目模板

为特定项目覆盖模板：

1. 打开项目
2. 前往**设置** → **AI**
3. 选择模板覆盖
4. 根据需要自定义

## 测试服务

### 连接测试

验证您的 API key 有效：

1. 点击服务上的**测试连接**
2. PackageFlow 发送简单请求
3. 显示成功或错误详情

### 延迟测试

检查响应时间：

1. 点击**测试延迟**
2. 计时多个请求
3. 显示平均延迟

<!-- TODO: Add screenshot of latency test results -->

## 默认服务

设置默认 AI 服务：

1. 前往**设置** → **AI 服务**
2. 点击服务旁的星号图标
3. 此服务在未特别指定时使用

## 使用限制

### 云端供应商

注意 API 速率限制和成本：

- OpenAI：按 token 计费
- Anthropic：按 token 计费
- Google：有免费层

### 本地供应商

本地运行时无限制：

- Ollama：无限制
- LM Studio：无限制

## 提示

1. **从 GPT-3.5 开始**：对大多数任务来说快速且便宜
2. **使用本地模型**：对于敏感代码，使用 Ollama
3. **自定义模板**：更好的提示 = 更好的结果
4. **测试连接**：在依赖 AI 功能前验证 API key 有效
5. **监控成本**：云端 API 调用会快速累积

## 疑难排解

### API Key 无效

- 验证 key 正确
- 检查 key 是否有必要权限
- 确保账单已启用（云端供应商）

### 响应缓慢

- 尝试较小/较快的模型
- 检查网络连接
- 考虑使用本地模型以获得更快响应

### 输出质量差

- 查看并改进您的提示模板
- 尝试更强大的模型
- 在模板中提供更多上下文
